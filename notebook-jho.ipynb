{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f5f35c6",
   "metadata": {},
   "source": [
    "## Baixa conteudo da web e salva em disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1d6fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baixa conteudo da web e salva em disco\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def create_directory_if_not_exists(directory_path):\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "def get_file_name(url:str, response:requests.Response) -> str:\n",
    "        file_name = 'download'\n",
    "        if 'Content-Disposition' in response.headers:\n",
    "            content_disposition = response.headers.get('Content-Disposition')\n",
    "            file_name = content_disposition.split('filename=')[1].strip('\"')\n",
    "        else:\n",
    "            file_name = url.split(\"/\")[-1]\n",
    "        return file_name\n",
    "\n",
    "def generate_available_filename(directory, filename):\n",
    "\n",
    "    name, extension = os.path.splitext(filename)\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    counter = 1\n",
    "\n",
    "    while os.path.exists(file_path):\n",
    "        new_filename = f\"{name} ({counter}){extension}\"\n",
    "        file_path = os.path.join(directory, new_filename)\n",
    "        counter += 1\n",
    "\n",
    "    return file_path\n",
    "\n",
    "def download(url:str, directory:str=os.getcwd()) -> str:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        file_name = generate_available_filename(directory, get_file_name(url, response))\n",
    "        create_directory_if_not_exists(directory)\n",
    "        with open(generate_available_filename(directory, get_file_name(url, response)), \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        return file_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49062341",
   "metadata": {},
   "source": [
    "## File Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdf753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def unzip(zip_path, dataset_unzip_directory, file_to_extract) -> str:\n",
    "     remove_file(os.path.join(dataset_unzip_directory, file_to_extract))\n",
    "     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "         return zip_ref.extract(file_to_extract, dataset_unzip_directory)\n",
    " \n",
    " \n",
    "def remove_file(caminho_arquivo):\n",
    "     if os.path.exists(caminho_arquivo):\n",
    "         os.remove(caminho_arquivo)\n",
    "     else:\n",
    "         print(f\"Arquivo '{caminho_arquivo}' não encontrado.\")   \n",
    "\n",
    "def move_file(origem, destino):\n",
    "     os.replace(origem, destino)\n",
    "\n",
    "def remove_directory(directory):\n",
    "     if os.path.exists(directory):\n",
    "         os.rmdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d5364",
   "metadata": {},
   "source": [
    "## CSV Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff2821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def convert_csv_to_parquet(csv_path):\n",
    "    \"\"\"Função para converter CSV em Parquet.\"\"\"\n",
    "    try:\n",
    "        # csv_path = convert_csv_encoding(csv_path, 'latin1', 'utf-8')\n",
    "        df = pd.read_csv(csv_path, sep=';', encoding='latin1', low_memory=False)\n",
    "        parquet_path = csv_path.replace(\".csv\", \".parquet\")\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        return parquet_path\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao converter {csv_path} para Parquet: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_csv_encoding(csv_path, source_encoding, target_encoding='utf-8') -> str:\n",
    "    \"\"\"Função para converter a codificação de um arquivo CSV.\"\"\"\n",
    "    try:\n",
    "        df = dd.read_csv(csv_path, encoding=source_encoding)\n",
    "        remove_file(csv_path)\n",
    "        df.to_csv(csv_path, sep=';', encoding=target_encoding, index=False)\n",
    "        return csv_path\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao converter a codificação do arquivo {csv_path}: {e}\")\n",
    "        raise e\n",
    "\n",
    "def convert_parquet_encoding(input_parquet_path, output_parquet_path, source_encoding='latin1', target_encoding='utf-8'):\n",
    "    df = pd.read_parquet(input_parquet_path)\n",
    "    \n",
    "    for col in df.select_dtypes(include=[object]):\n",
    "        df[col] = df[col].apply(lambda x: x.encode(source_encoding).decode(target_encoding) if isinstance(x, str) else x)\n",
    "    \n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, output_parquet_path)\n",
    "    \n",
    "    print(f\"Arquivo Parquet convertido de Latin1 para UTF-8 e salvo em {output_parquet_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9530f",
   "metadata": {},
   "source": [
    "## Prepara datasets para serem ingeridos pelo notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99758ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "resources_directory = os.path.join(os.getcwd(), \"resources\")\n",
    "\n",
    "dataset_directory = os.path.join(os.getcwd(), \"dataset/2022\")\n",
    "dataset_unzip_directory= os.path.join(dataset_directory, \"unzip\")\n",
    "\n",
    "\n",
    "## DataSet Principal (Candidatos)\n",
    "resource_file = os.path.join(resources_directory, \"consulta_cand_2022.zip\")\n",
    "if not os.path.exists(resource_file):\n",
    "    URL = 'https://cdn.tse.jus.br/estatistica/sead/odsele/consulta_cand/consulta_cand_2022.zip'\n",
    "    resource_file = download(URL, resources_directory)\n",
    "\n",
    "file_to_extract = \"consulta_cand_2022_BRASIL.csv\"\n",
    "file_extracted = unzip(resource_file, dataset_unzip_directory, file_to_extract)\n",
    "parquet_file = convert_csv_to_parquet(file_extracted)\n",
    "if parquet_file:\n",
    "    remove_file(file_extracted)\n",
    "    # convert_parquet_encoding(parquet_file, os.path.join(dataset_directory, \"candidatos.parquet\"))\n",
    "    move_file(parquet_file, os.path.join(dataset_directory, \"candidatos.parquet\"))\n",
    "\n",
    "## DataSet Complementar (Informacoes Complementares)\n",
    "resource_file = os.path.join(resources_directory, \"consulta_cand_complementar_2022.zip\")\n",
    "if not os.path.exists(resource_file):\n",
    "    URL = 'https://cdn.tse.jus.br/estatistica/sead/odsele/consulta_cand_complementar/consulta_cand_complementar_2022.zip'\n",
    "    resource_file = download(URL, resources_directory)\n",
    "\n",
    "file_to_extract = \"consulta_cand_complementar_2022_BRASIL.csv\"\n",
    "file_extracted = unzip(resource_file, dataset_unzip_directory, file_to_extract)\n",
    "parquet_file = convert_csv_to_parquet(file_extracted)\n",
    "if parquet_file:\n",
    "    remove_file(file_extracted)\n",
    "    move_file(parquet_file, os.path.join(dataset_directory, \"candidatos.infos_adicionais.parquet\"))\n",
    "\n",
    "\n",
    "## DataSet Bens dos Candidatos\n",
    "resource_file = os.path.join(resources_directory, \"bem_candidato_2022.zip\")\n",
    "if not os.path.exists(resource_file):\n",
    "    URL = 'https://cdn.tse.jus.br/estatistica/sead/odsele/bem_candidato/bem_candidato_2022.zip'\n",
    "    resource_file = download(URL, resources_directory)\n",
    "\n",
    "file_to_extract = \"bem_candidato_2022_BRASIL.csv\"\n",
    "file_extracted = unzip(resource_file, dataset_unzip_directory, file_to_extract)\n",
    "parquet_file = convert_csv_to_parquet(file_extracted)\n",
    "if parquet_file:\n",
    "    remove_file(file_extracted)\n",
    "    move_file(parquet_file, os.path.join(dataset_directory, \"candidatos.bens.parquet\"))\n",
    "\n",
    "\n",
    "\n",
    "## DataSet Prestação de Contas\n",
    "resource_file = os.path.join(resources_directory, \"prestacao_de_contas_eleitorais_candidatos_2022.zip\")\n",
    "if not os.path.exists(resource_file):\n",
    "    URL = 'https://cdn.tse.jus.br/estatistica/sead/odsele/prestacao_contas/prestacao_de_contas_eleitorais_candidatos_2022.zip'\n",
    "    resource_file = download(URL, resources_directory)\n",
    "\n",
    "\n",
    "\n",
    "## Despesas Contratadas\n",
    "file_to_extract = \"despesas_contratadas_candidatos_2022_BRASIL.csv\"\n",
    "file_extracted = unzip(resource_file, dataset_unzip_directory, file_to_extract)\n",
    "parquet_file = convert_csv_to_parquet(file_extracted)\n",
    "if parquet_file:\n",
    "    remove_file(file_extracted)\n",
    "    move_file(parquet_file, os.path.join(dataset_directory, \"candidatos.despesas.contratadas.parquet\"))\n",
    "\n",
    "## Despesas Contratadas Pagas\n",
    "file_to_extract = \"despesas_pagas_candidatos_2022_BRASIL.csv\"\n",
    "file_extracted = unzip(resource_file, dataset_unzip_directory, file_to_extract)\n",
    "parquet_file = convert_csv_to_parquet(file_extracted)\n",
    "if parquet_file:\n",
    "    remove_file(file_extracted)\n",
    "    move_file(parquet_file, os.path.join(dataset_directory, \"candidatos.despesas.pagas.parquet\"))\n",
    "\n",
    "## Receitas\n",
    "file_to_extract = \"receitas_candidatos_2022_BRASIL.csv\"\n",
    "file_extracted = unzip(resource_file, dataset_unzip_directory, file_to_extract)\n",
    "parquet_file = convert_csv_to_parquet(file_extracted)\n",
    "if parquet_file:\n",
    "    remove_file(file_extracted)\n",
    "    move_file(parquet_file, os.path.join(dataset_directory, \"candidatos.receitas.parquet\"))\n",
    "\n",
    "## Receitas\n",
    "file_to_extract = \"receitas_candidatos_doador_originario_2022_BRASIL.csv\"\n",
    "file_extracted = unzip(resource_file, dataset_unzip_directory, file_to_extract)\n",
    "parquet_file = convert_csv_to_parquet(file_extracted)\n",
    "if parquet_file:\n",
    "    remove_file(file_extracted)\n",
    "    move_file(parquet_file, os.path.join(dataset_directory, \"candidatos.receitas.doador_originario.parquet\"))\n",
    "\n",
    "remove_directory(dataset_unzip_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "resources_directory = os.path.join(os.getcwd(), \"resources\")\n",
    "\n",
    "dataset_directory = os.path.join(os.getcwd(), \"dataset/2022\")\n",
    "dataset_unzip_directory= os.path.join(dataset_directory, \"unzip\")\n",
    "\n",
    "\n",
    "## DataSet Resultados\n",
    "resource_file = os.path.join(resources_directory, \"votacao_candidato_munzona_2022.zip\")\n",
    "if not os.path.exists(resource_file):\n",
    "    URL = 'https://cdn.tse.jus.br/estatistica/sead/odsele/votacao_candidato_munzona/votacao_candidato_munzona_2022.zip'\n",
    "    resource_file = download(URL, resources_directory)\n",
    "\n",
    "file_to_extract = \"votacao_candidato_munzona_2022_BRASIL.csv\"\n",
    "file_extracted = unzip(resource_file, dataset_unzip_directory, file_to_extract)\n",
    "parquet_file = convert_csv_to_parquet(file_extracted)\n",
    "if parquet_file:\n",
    "    remove_file(file_extracted)\n",
    "    move_file(parquet_file, os.path.join(dataset_directory, \"resultados.votacao.canditados.parquet\"))\n",
    "\n",
    "remove_directory(dataset_unzip_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda3f94",
   "metadata": {},
   "source": [
    "# Preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a344730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SQ_CANDIDATO  NR_CANDIDATO                      NM_CANDIDATO  \\\n",
      "0      100001608211          5120    BRUNNA DE SOUZA AMORIM MARCIEL   \n",
      "1      250001619350         50110        EDIANE MARIA DO NASCIMENTO   \n",
      "2       70001723597         20777    MARIA JOSÉ ALVES MORENO CABRAL   \n",
      "3       80001719700         65077    CARMEM LUCIA GOMES DE OLIVEIRA   \n",
      "4      190001619212          1388               ROBSON SOUZA SANTOS   \n",
      "...             ...           ...                               ...   \n",
      "29309  130001634154          1940              JOSE MARCIO DOS REIS   \n",
      "29310  240001614291         14000          JULIANO DA SILVA MARTINS   \n",
      "29311  240001644409         12212         ROSELÉIA LUCAS DOS SANTOS   \n",
      "29312  140001596650            22    ROSIANE CHAGAS MESQUITA EGUCHI   \n",
      "29313  170001736575         14300  GERCINALDO DO NASCIMENTO BARBOZA   \n",
      "\n",
      "      NM_URNA_CANDIDATO  CD_SITUACAO_CANDIDATURA  NR_PARTIDO SG_PARTIDO  \\\n",
      "0        BRUNNA MARCIEL                       12          51   PATRIOTA   \n",
      "1          EDIANE MARIA                       12          50       PSOL   \n",
      "2       MARIA DA ADIRPP                       12          20        PSC   \n",
      "3          CARMEM LUCIA                       12          65    PC do B   \n",
      "4          BINHO FAVELA                       12          13         PT   \n",
      "...                 ...                      ...         ...        ...   \n",
      "29309    ZE MARCIO REIS                       12          19       PODE   \n",
      "29310   JULIANO MARTINS                       12          14        PTB   \n",
      "29311                RÔ                       12          12        PDT   \n",
      "29312    ROSIANE EGUCHI                        3          22         PL   \n",
      "29313       IRMÃO TUTUI                        3          14        PTB   \n",
      "\n",
      "      SG_UF_NASCIMENTO DT_NASCIMENTO  CD_GENERO  ... CD_GRAU_INSTRUCAO  \\\n",
      "0                   PI    11/06/1982          4  ...                 6   \n",
      "1                   PE    10/10/1983          4  ...                 2   \n",
      "2                   MG    07/04/1964          4  ...                 6   \n",
      "3                   AL    09/04/1977          4  ...                 8   \n",
      "4                   RJ    07/08/1981          2  ...                 6   \n",
      "...                ...           ...        ...  ...               ...   \n",
      "29309               MG    31/12/1983          2  ...                 6   \n",
      "29310               SC    13/02/1981          2  ...                 8   \n",
      "29311               SC    03/05/1980          4  ...                 7   \n",
      "29312   Não divulgável          None         -4  ...                -4   \n",
      "29313   Não divulgável          None         -4  ...                -4   \n",
      "\n",
      "           DS_GRAU_INSTRUCAO CD_ESTADO_CIVIL  DS_ESTADO_CIVIL CD_COR_RACA  \\\n",
      "0      ENSINO MÉDIO COMPLETO               3        CASADO(A)           1   \n",
      "1               LÊ E ESCREVE               1      SOLTEIRO(A)           2   \n",
      "2      ENSINO MÉDIO COMPLETO               3        CASADO(A)           3   \n",
      "3          SUPERIOR COMPLETO               1      SOLTEIRO(A)           2   \n",
      "4      ENSINO MÉDIO COMPLETO               1      SOLTEIRO(A)           3   \n",
      "...                      ...             ...              ...         ...   \n",
      "29309  ENSINO MÉDIO COMPLETO               1      SOLTEIRO(A)           1   \n",
      "29310      SUPERIOR COMPLETO               3        CASADO(A)           1   \n",
      "29311    SUPERIOR INCOMPLETO               1      SOLTEIRO(A)           1   \n",
      "29312         NÃO DIVULGÁVEL              -4   NÃO DIVULGÁVEL          -4   \n",
      "29313         NÃO DIVULGÁVEL              -4   NÃO DIVULGÁVEL          -4   \n",
      "\n",
      "          DS_COR_RACA CD_OCUPACAO  \\\n",
      "0              BRANCA         595   \n",
      "1               PRETA         598   \n",
      "2               PARDA         598   \n",
      "3               PRETA         598   \n",
      "4               PARDA         599   \n",
      "...               ...         ...   \n",
      "29309          BRANCA         593   \n",
      "29310          BRANCA         593   \n",
      "29311          BRANCA         593   \n",
      "29312  NÃO DIVULGÁVEL          -4   \n",
      "29313  NÃO DIVULGÁVEL          -4   \n",
      "\n",
      "                                        DS_OCUPACAO CD_SIT_TOT_TURNO  \\\n",
      "0      AGENTE DE SERVIÇOS FUNERÁRIOS E EMBALSAMADOR                5   \n",
      "1                               EMPREGADO DOMÉSTICO                2   \n",
      "2                               EMPREGADO DOMÉSTICO                4   \n",
      "3                               EMPREGADO DOMÉSTICO                5   \n",
      "4                                   GUIA DE TURISMO                5   \n",
      "...                                             ...              ...   \n",
      "29309                                   DESPACHANTE                5   \n",
      "29310                                   DESPACHANTE                5   \n",
      "29311                                   DESPACHANTE                5   \n",
      "29312                                NÃO DIVULGÁVEL               -1   \n",
      "29313                                NÃO DIVULGÁVEL               -1   \n",
      "\n",
      "       DS_SIT_TOT_TURNO  \n",
      "0              SUPLENTE  \n",
      "1         ELEITO POR QP  \n",
      "2            NÃO ELEITO  \n",
      "3              SUPLENTE  \n",
      "4              SUPLENTE  \n",
      "...                 ...  \n",
      "29309          SUPLENTE  \n",
      "29310          SUPLENTE  \n",
      "29311          SUPLENTE  \n",
      "29312            #NULO#  \n",
      "29313            #NULO#  \n",
      "\n",
      "[29314 rows x 21 columns]\n",
      "View materializada e salva com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Criar uma conexão DuckDB em memória\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Carregar os dois arquivos Parquet\n",
    "con.execute(\"CREATE VIEW candidatos AS SELECT * FROM 'dataset/2022/candidatos.parquet'\")\n",
    "con.execute(\"CREATE VIEW candidatos_bens AS SELECT * FROM 'dataset/2022/candidatos.bens.parquet'\")\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        candidato.SQ_CANDIDATO,\n",
    "        candidato.NM_CANDIDATO,\n",
    "        candidato.NM_URNA_CANDIDATO,\n",
    "        candidato.NR_CANDIDATO,\n",
    "        candidato.CD_SITUACAO_CANDIDATURA,\n",
    "        bem.*\n",
    "    FROM candidatos cantidato\n",
    "    inner join candidatos_bens bem on bem.SQ_CANDIDATO = cantidato.SQ_CANDIDATO\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Executar a consulta para combinar as tabelas (JOIN, UNION, etc.)\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        candidatos.SQ_CANDIDATO,\n",
    "        candidatos.NR_CANDIDATO,\n",
    "        candidatos.NM_CANDIDATO,\n",
    "        candidatos.NM_URNA_CANDIDATO,\n",
    "        candidatos.CD_SITUACAO_CANDIDATURA,\n",
    "        candidatos.NR_PARTIDO,\n",
    "        candidatos.SG_PARTIDO,\n",
    "        candidatos.SG_UF_NASCIMENTO,\n",
    "        candidatos.DT_NASCIMENTO,\n",
    "        candidatos.CD_GENERO,\n",
    "        candidatos.DS_GENERO,\n",
    "        candidatos.CD_GRAU_INSTRUCAO,\n",
    "        candidatos.DS_GRAU_INSTRUCAO,\n",
    "        candidatos.CD_ESTADO_CIVIL,\n",
    "        candidatos.DS_ESTADO_CIVIL,\n",
    "        candidatos.CD_COR_RACA,\n",
    "        candidatos.DS_COR_RACA,\n",
    "        candidatos.CD_OCUPACAO,\n",
    "        candidatos.DS_OCUPACAO,\n",
    "        candidatos.CD_SIT_TOT_TURNO,\n",
    "        candidatos.DS_SIT_TOT_TURNO,\n",
    "    FROM candidatos\n",
    "\"\"\"\n",
    "# Obter o resultado como um DataFrame\n",
    "df_resultante = con.execute(query).fetchdf()\n",
    "\n",
    "print(df_resultante)\n",
    "\n",
    "views_directory = os.path.join(os.getcwd(), \"dataset\", \"views\")\n",
    "os.makedirs(views_directory, exist_ok=True)\n",
    "\n",
    "# Salvar o resultado em um novo arquivo Parquet\n",
    "df_resultante.to_parquet(os.path.join(views_directory, \"candidatos.dados.base.parquet\"), index=False)\n",
    "\n",
    "print(\"View materializada e salva com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a41ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Caminhos dos arquivos Parquet\n",
    "arquivos_parquet = [\n",
    "    'dataset/2022/candidatos.bens.parquet',\n",
    "    'dataset/2022/candidatos.despesas.contratadas.parquet',\n",
    "    'dataset/2022/candidatos.despesas.pagas.parquet',\n",
    "    'dataset/2022/candidatos.infos_adicionais.parquet',\n",
    "    'dataset/2022/candidatos.parquet',\n",
    "    'dataset/2022/candidatos.receitas.doador_originario.parquet',\n",
    "    'dataset/2022/candidatos.receitas.parquet',\n",
    "    'dataset/2022/resultados.votacao.canditados.parquet'\n",
    "]\n",
    "\n",
    "# Conectar ao DuckDB e salvar o banco em um arquivo .db\n",
    "con = duckdb.connect('meu_banco.db')\n",
    "\n",
    "# Iterar sobre os arquivos Parquet e inserir os dados em uma tabela no banco .db\n",
    "for arquivo in arquivos_parquet:\n",
    "    # Ler o arquivo Parquet e inserir no banco DuckDB\n",
    "    con.execute(f\"COPY (SELECT * FROM '{arquivo}') TO 'meu_banco.db' (FORMAT PARQUET)\")\n",
    "    print(f\"Arquivo {arquivo} inserido com sucesso no banco!\")\n",
    "\n",
    "# Verificar o conteúdo da tabela (opcional)\n",
    "resultado = con.execute(\"SELECT * FROM 'meu_banco.db' LIMIT 10\").fetchdf()\n",
    "print(resultado)\n",
    "\n",
    "# Fechar a conexão\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c98ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Ler o esquema do arquivo Parquet\n",
    "arquivo_parquet = 'dataset/2022/candidatos.parquet'\n",
    "parquet_file = pq.ParquetFile(arquivo_parquet)\n",
    "\n",
    "# Exibir o esquema do arquivo Parquet\n",
    "print(parquet_file.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ecb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Diretório onde os arquivos .parquet estão localizados\n",
    "diretorio_parquet = 'dataset/2022'\n",
    "# Conecta ao banco de dados SQLite (ou cria um novo)\n",
    "conn = sqlite3.connect('seu_banco_de_dados.sqlite')\n",
    "\n",
    "# Itera sobre os arquivos .parquet no diretório\n",
    "for arquivo in os.listdir(diretorio_parquet):\n",
    "    if arquivo.endswith('.parquet'):\n",
    "        # Lê o arquivo .parquet\n",
    "        caminho_completo = os.path.join(diretorio_parquet, arquivo)\n",
    "        df = pd.read_parquet(caminho_completo)\n",
    "        \n",
    "        # Define o nome da tabela com base no nome do arquivo\n",
    "        nome_tabela = os.path.splitext(arquivo)[0]  # Remove a extensão .parquet\n",
    "\n",
    "        # Insere os dados no banco de dados\n",
    "        df.to_sql(nome_tabela, conn, if_exists='replace', index=False)\n",
    "\n",
    "# Fecha a conexão\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpad-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
